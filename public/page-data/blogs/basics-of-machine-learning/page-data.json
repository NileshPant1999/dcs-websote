{"componentChunkName":"component---src-templates-blog-template-js","path":"/blogs/basics-of-machine-learning","result":{"data":{"blog":{"content":"## Linear Regression From Scratch\n## \n### Linear regression most basic concept of machine learning\n\n![1.png](/uploads/1_307ed1675e.png)\n\nMachine learning is a subset of artificial intelligence commonly known as AI is a way that provides the computer the ability to learn and improve from the experience without being explicitly programmed. Machine learning comprises various algorithms but the most basic one was linear regression. It is a type of supervised machine learning algorithm which means the data on which you are going to use the algorithm is labeled. So before getting started there are some prerequisites that you must have that is some background knowledge of school level linear algebra and calculus.\n\n### What basically is Linear Regression?\n\nAs the name suggests linear regression basically used for establishing a relationship between two variables and if the number of variables is greater then two then it is called multivariate linear regression. We will talk only about linear regression in this article. Consider it as you give one input to the algorithm say the height of a person and it will find that how this height is related to the weight of the person. \n\nWe have all know the equation of a line in a 2D plane is given by y=mx +c where m is the slope of the line and c is the constant. Basically, what linear regression does is it found the best value of m and c which will best fit the data points. Basically it will guess some random value of m then check how incorrect it was and then manually adjust itself to become slightly more accurate. During Linear regression, this process is repeated constantly.\n\n### Steps involved in Linear Regression\n1. Initializing random parameters for the defined function\n2. Calculating the error\n3. Computing the partial derivatives\n4. Updating the parameters based on learning rate and partial derivative\n5. Minimizing the cost function.\n6. We will be talking about each step one by one\n\n### Step 1: Initializing random parameter for Hypothesis function\n\nWe see the equation y = mx + c above the Hypothesis function is the same as the above equation but with slightly different notations.\n\n![3.png](/uploads/3_cc681a142c.png)\n\nWhat you can see is that we only change m and c by θ₁ and θ₀ which are the parameters of the hypothesis function. Let see how does it look in code\n\n```\ndef hypothesis(theta_0,theta_1,x):\n    return (theta_1*x)+ theta_0\n```\nAt the starting stage we will randomly initialize our parameter i.e, our parameter can choose any value randomly to begin and when we use this function on our data and plot the graph it will look something like this\n\n![https://miro.medium.com/max/1000/0*xnIc0j4XgiZWsa79.jpeg](/uploads/0_xn_Ic0j4_Xgi_Z_Wsa79_305740fdd5.jpeg)\n\nTo check this hypothesis let’s create a sample data-set using numpy and plot it using matplotlib.\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = [5,7,8,7,2,17,2,9,4,11,12,9,6]\ny = [99,86,87,88,111,86,103,87,94,78,77,85,86]\nplt.scatter(x,y)\nplt.show()\n```\nwhen we run the above code it will look something like this:\n![https://miro.medium.com/max/750/1*JysU5E7Nt7N98XELPxrE5A.png](/uploads/1_Jys_U5_E7_Nt7_N98_XEL_Pxr_E5_A_6dffa20bc0.png)\n\nAfter this we will create our hypothesis function with random initialization and test it on the given data points.\n\n```\ndef plotLine(theta0, theta1, x, y):\n    max_x = np.max(x) + 100\n    min_x = np.min(x) - 100\n    xplot = np.linspace(min_x, max_x, 1000)\n    yplot = theta0 + theta1 * xplot\n    plt.plot(xplot, yplot,color='Red')\n    plt.scatter(x,y)\n    plt.show()\ntheta0 = np.random.rand()\ntheta1 = np.random.rand()\nplotLine(theta0, theta1, x, y)\n\n```\nThe output of the above function is \n\n![https://miro.medium.com/max/754/1*yO1ZIiePZn9u6AaLmd-_Rg.png](/uploads/1_y_O1_Z_Iie_P_Zn9u6_Aa_Lmd_Rg_9e03d74c26.png)\n\nwe can see that the line does not fit the data at all because it is a random line. But we need to do something to get it right. That is where Error Function aur we can say that cost function comes in play.\n\n### Step 2: Calculating the error\n\nIn the above step we can clearly see that our line is wrong but the question is how wrong it is. The answer to this question Error function. This function is responsible for calculating the total error the model made. There are many type error function that are used but we will be using mean square error. The cost function or error function is represented by J.\n\n\n\nWhat the above equation is doing is it was calculating the error by subtracting the actual value from the predicted one and then take the square of this value for all the data points and then take the sum of error of all data points and divide it by two. We are squaring the error because error cannot be negative and we are dividing the equation by 2 because it will help us in further calculation. If you know about derivatives you know what I mean.\n\n\nHere what the cost function looks like in code\n```\ndef cost(theta0,theta1,x,y):\n    cost_value = 0\n    for (i,j) in (x,y):\n        cost_value += ((hypothesis(theta0,theta1,i)-j)**2) / 2\n    return cost_value\n```\nNow that we have calculated the error it is time for step 3 to come:\n![https://miro.medium.com/max/1400/1*AVztMosI0iasTrArvi0CxQ.png](/uploads/1_A_Vzt_Mos_I0ias_Tr_Arvi0_Cx_Q_903d1bc711.png)\n\n### STEP 3 : Calculating the Partial Derivatives\nSo in step 2 we calculate the error our model is making and we need to minimize this error but one of the most import question here is where to move this sound complicated but it’s not let us understand it graphically. When we plot our error function with the parameter we get a graph something like this:\n![https://miro.medium.com/max/1400/1*RFr-bu4BODVb8GDEukCwbA.png](/uploads/1_R_Fr_bu4_BOD_Vb8_GD_Euk_Cwb_A_ef1f81d17e.png)\n\nOn observing this graph we can say that error is minimum at the lowest point of the graph. So the answer to the question where to move is move towards the lowest point of the graph. The process of finding this point is minimizing the cost function.\nWhat we can say is that at bottom of the graph the gradient is zero. So the final goal of our model should be to make the gradient zero. Now another question arises what is gradient?\nGradient is given by the partial derivatives of the cost function with respect to the parameters. Partial derivatives of the cost function are:\n\n![https://miro.medium.com/max/1400/1*hniMbdqZIvt43JW5wMOqTA.png](/uploads/1_hni_Mbdq_Z_Ivt43_JW_5w_M_Oq_TA_ccd539626e.png)\n\nWe can calculate the partial derivative of the cost using the following function\n\n```\ndef derivatives(theta0,theta1,x,y):\n    dtheta0 = 0\n    dtheta1 = 0\n    for i,j in range (x,y):\n        dtheta0 += (hypothesis(theta0,theta1,i) - j\n        dtheta1 += ((hypothesis(theta0,theta1,i) - j) * i\n    dtheta0 /= len(x)\n    dtheta1 /= len(x)\n    \n    return dtheta0,dtheta1\n\n```\n### STEP 4: Updating the parameters based on learning rate and partial derivatives\nNow the final step is to update the parameter to reduce the gradient. After step 3 and step 4 we know how much to move and where to move. Now is the time to update the parameters. We do this by using the gradient update rule which is\n\n\n\n![https://miro.medium.com/max/1336/0*ck-9hsjYHYUU2vey.png](/uploads/0_ck_9hsj_YHYUU_2vey_6328c5fa24.png)\n\nThe above equation is used to update the parameters. We encountered a new term here which is Alpha (α) . It is what we call learning rate which determines the step size at each iteration while moving towards the minimum loss function or we can say that moving towards the point where the gradient is zero.\n\n![https://miro.medium.com/max/1012/1*heQR61fa32f8OzjYSyE5tg.png](/uploads/1_he_QR_61fa32f8_Ozj_Y_Sy_E5tg_c1bc6b4023.png)\n\nwe need to carefully select the learning rate because as shown in 1 if the learning rate is too small it may take forever to converge while on the other hand if the learning rate is too large it may overshoot the point where the gradient is zero.\n\n```\ndef update_parameter(theta0,theta1,x,y,alpha):\n    dtheta0, dtheta1 = derivatives(theta0, theta1, x, y)\n    theta0 = theta0 - (alpha * dtheta0)\n    theta1 = theta1 - (alpha * dtheta1)\n     \n    return theta0,theta1\n\n```\n### Step 5: Minimizing the cost function\nNow we repeat step 2, step 3, step 4 until the error is as low as possible this step is called minimizing the cost function. We can know to combine all our code in one function which is linear regression\n\n```\ndef LinearRegression(x,y):\n    theta0 = np.random.rand()\n    theta1 - np.random.rand()\n    for i in range(0,1000):\n        if i%100==0:\n            plotLine(theta0,theta1,x,y)\n        theta0,theta1 = updata_parameter(theta0,theta1,x,y,0.001)\n\n```\nAfter applying the Linear Regression model we get a result something like this:\n\n![https://miro.medium.com/max/960/1*KcTlouorn1nkfDrJjrPrfQ.gif](/uploads/1_Kc_Tlouorn1nkf_Dr_Jjr_Prf_Q_45ad6ec8e7.gif)\n\nWritten By\nNilesh Pant"}},"pageContext":{"slug":"basics-of-machine-learning"}}}